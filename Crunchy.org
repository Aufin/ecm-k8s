#+TITLE: Crunchy Postgres for Kubernetes

We need to be able to mount other file systems within the postgresql
cluster. CloundNativePG does not allow such things.



* The first =postgres.yaml= file

[[file:kustomize/ecm-db/postgres.yaml][here]]

#+begin_src sh :tangle crunchy.sh
  alias minikube='sudo minikube'
  alias kubectl='sudo kubectl'

  mrestart () {
  	minikube delete
  	# sudo and minikube make locks strange
  	sudo sysctl fs.protected_regular=0
  	rm -v /tmp/juju-mk*

  	minikube start --disk-size=64g --force
      minikube mount --uid 1001 --gid 1001 /srv/share-test:/share &
   
  }

  k-setup () {
  	csi-setup && \
  		k-ecm-namespace && \
  		k-ecm-storage && \
  		apply-CPK
  }

  k-nfs-setup () {
    k-ecm-nfs-service
    echo Waiting for it to be ready ...
    kubectl wait --for=condition=Ready pod/nfs-server-0 --timeout=60s
    k-ecm-nfs-storage
  }

  k-db-setup () {
   kubectl apply -f ecm-db-shared-volume.yaml
   #echo waiting for pvc to be bound
   #kubectl wait --for=jsonpath='{.status.phase}'=Bound pvc/pvc-ecm-shared-with-db
   apply-ecm-db
  }		

  k-db-init () {
   DUMP=${1:-db-dump-2025-10-10.sql}
   export PG_CLUSTER_PRIMARY_POD=$(kubectl get pod -o name -l postgres-operator.crunchydata.com/cluster=ecm-db,postgres-operator.crunchydata.com/role=master)
   kubectl exec $PG_CLUSTER_PRIMARY_POD -- psql --echo-all -f /tablespaces/shared/$DUMP
  }

  csi-setup () {
       cd csi-driver-nfs
       sudo ./deploy/install-driver.sh v4.12.1 local
       cd -
  }

  nfs-pod () {
    kubectl get pod -lapp=nfs-server -oname
  }

  k-ecm-namespace  () {
  	kubectl apply -f ecm-namespace.yaml
      kubectl config set-context --current --namespace=ecm
      
  }

  k-ecm-storage () {
  	kubectl apply -f local-persistent-volumes.yaml
  	kubectl apply -f local-storage-classes.yaml
      
  }

  k-ecm-nfs-service () {
  	kubectl apply -f ecm-global-pvc.yaml
      kubectl apply -n ecm -f ecm-nfs-deployment.yaml 
      kubectl apply -n ecm -f ecm-nfs-service.yaml
  }

  k-ecm-nfs-storage () {
  	kubectl apply -f ecm-shared-class.yaml
  	kubectl apply -f ecm-shared-volume.yaml
  	kubectl apply -f ecm-shared-claim.yaml
  }



  apply-CPK () {
  	kubectl apply -k kustomize/install/namespace
  	kubectl apply --server-side -k kustomize/install/default/
  }

  apply-ecm-db () {
  	kubectl apply -k kustomize/ecm-db/
  }

  export-kenv () {
      export PGO_POD=$(kubectl get -n postgres-operator -o name -l app.kubernetes.io/name=pgo pod)
  	export PG_CLUSTER_INSTANCE_1=$(kubectl get pod -n postgres-operator -o name -l postgres-operator.crunchydata.com/cluster=ecm-db,postgres-operator.crunchydata.com/instance-set=instance1)
  }




  do-over () {
  	mrestart
  	kstorage
  	apply-CPK
      sleep 1
  	export-kenv
  	while [ -z $PGO_POD ]; do echo no pod? ; kubectl get -n postgres-operator pod; sleep 1; export-kenv ; done;
  	kubectl wait --for=condition=Ready $PGO_POD --timeout=60s
  	apply-ecm-db
  	export-kenv
  }


  export SHARED_VOLUME_NAME=$(kubectl get -n postgres-operator -o=jsonpath='{.items[*].spec.volumeName}' -l postgres-operator.crunchydata.com/data=shared,postgres-operator.crunchydata.com/cluster=ecm-db,postgres-operator.crunchydata.com/role=tablespace pvc)

  # kubectl label pv $SHARED_VOLUME_NAME shared-ecm-db=pv

#+end_src

#+begin_src sh
  export PG_CLUSTER_NAME=ecm-db
  export PG_CLUSTER_PRIMARY_POD=$(kubectl get pod -n postgres-operator -o name -l postgres-operator.crunchydata.com/cluster=$PG_CLUSTER_NAME,postgres-operator.crunchydata.com/role=master)
  export PGUSER=postgres
  unset PGDATABASE
  export PGPASSWORD=$(kubectl get secrets -n postgres-operator $PG_CLUSTER_NAME-pguser-postgres -o go-template='{{.data.password | base64decode}}')
#+end_src

Forward and see.

#+begin_src sh
  kubectl -n postgres-operator port-forward "${PG_CLUSTER_PRIMARY_POD}" 5433:5432 &
  # port forwarding takes a bit
  sleep 2;
  psql -h localhost -p 5433
  
  psql -h localhost -p 5433 --echo-all -f ~/db-dump-2025-10-10.sql
#+end_src

 
* The Shared Volumes

#+begin_src yaml :tangle ecm-db-shared-volume.yaml
  ---
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    annotations:
      pv.kubernetes.io/provisioned-by: nfs.csi.k8s.io
    name: pv-ecm-shared-with-db-1
    namespace: ecm
  spec:
    capacity:
      storage: 1Ti
    accessModes:
      - ReadWriteMany
    persistentVolumeReclaimPolicy: Retain
    storageClassName: ecm-shared
    mountOptions:
      - nfsvers=4.1
    csi:
      driver: nfs.csi.k8s.io
      # volumeHandle format: {nfs-server-address}#{sub-dir-name}#{share-name}
      # make sure this value is unique for every share in the cluster
      volumeHandle: nfs-service.ecm.svc.cluster.local/share##
      volumeAttributes:
        server: nfs-service.ecm.svc.cluster.local
        share: /data
  ---
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    annotations:
      pv.kubernetes.io/provisioned-by: nfs.csi.k8s.io
    name: pv-ecm-shared-with-db-2
    namespace: ecm
  spec:
    capacity:
      storage: 1Ti
    accessModes:
      - ReadWriteMany
    persistentVolumeReclaimPolicy: Retain
    storageClassName: ecm-shared
    mountOptions:
      - nfsvers=4.1
    csi:
      driver: nfs.csi.k8s.io
      # volumeHandle format: {nfs-server-address}#{sub-dir-name}#{share-name}
      # make sure this value is unique for every share in the cluster
      volumeHandle: nfs-service.ecm.svc.cluster.local/share2##
      volumeAttributes:
        server: nfs-service.ecm.svc.cluster.local
        share: /data
  ---
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    annotations:
      pv.kubernetes.io/provisioned-by: nfs.csi.k8s.io
    name: pv-db-backrest
    namespace: ecm
  spec:
    capacity:
      storage: 1Ti
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Retain
    storageClassName: ecm-shared
    mountOptions:
      - nfsvers=4.1
    csi:
      driver: nfs.csi.k8s.io
      # volumeHandle format: {nfs-server-address}#{sub-dir-name}#{share-name}
      # make sure this value is unique for every share in the cluster
      volumeHandle: nfs-service.ecm.svc.cluster.local/share##
      volumeAttributes:
        server: nfs-service.ecm.svc.cluster.local
        share: /pg-backrest
#+end_src

* Change the namespace

To modify the name of namespace created by the installer, the
=kustomize/install/namespace/namespace.yaml= should be modified:

#+begin_src sh :results verbatim :wrap src diff
  git diff kustomize/install/namespace/namespace.yaml
#+end_src

#+begin_src diff
diff --git a/kustomize/install/namespace/namespace.yaml b/kustomize/install/namespace/namespace.yaml
index bfebd8ac..d435fa1c 100644
--- a/kustomize/install/namespace/namespace.yaml
+++ b/kustomize/install/namespace/namespace.yaml
@@ -1,4 +1,4 @@
 apiVersion: v1
 kind: Namespace
 metadata:
-  name: postgres-operator
+  name: ecm
#+end_src


The namespace setting in =kustomize/install/default/kustomization.yaml= 
should be modified accordingly.

#+begin_src sh :results verbatim :wrap src diff
  git diff kustomize/install/default/kustomization.yaml
#+end_src

#+RESULTS:
#+begin_src diff
diff --git a/kustomize/install/default/kustomization.yaml b/kustomize/install/default/kustomization.yaml
index e7a0ea52..f6bb66ae 100644
--- a/kustomize/install/default/kustomization.yaml
+++ b/kustomize/install/default/kustomization.yaml
@@ -1,6 +1,6 @@
 kind: Kustomization
 
-namespace: postgres-operator
+namespace: ecm
 
 labels:
 - includeSelectors: false
@@ -34,4 +34,4 @@ patches:
           - name: operator
             env:
             - name: PGO_FEATURE_GATES
#+end_src

* The "Tablespace" to share a folder

"A Tablespace is a Postgres feature that is used to store data on a
different volume than the primary data directory.

Some examples of use cases for tablespaces include:

    Putting data onto archival systems " -- [[https://access.crunchydata.com/documentation/postgres-operator/latest/guides/tablespaces][docs]]


We want a folder that can be used to communicate between all pods. The
easy way to do that is mounting, but not using, a tablespace.

We have a =StorageClass= named =ecm-shared=. We'll put the two
together.

** Add the Feature

In the kustomize/install/default/kustomization.yaml file repository you will see a section like this:

  #+begin_src yaml
    env:
    - name: PGO_FEATURE_GATES
  #+end_src

  
We want this: =PGO_FEATURE_GATES="TablespaceVolumes=true"=

#+begin_src sh :results verbatim :wrap src diff
  git diff kustomize/install/default/kustomization.yaml
#+end_src

So there you go.

#+begin_src diff
diff --git a/kustomize/install/default/kustomization.yaml b/kustomize/install/default/kustomization.yaml
index e7a0ea5..881aa74 100644
--- a/kustomize/install/default/kustomization.yaml
+++ b/kustomize/install/default/kustomization.yaml
@@ -34,4 +34,4 @@ patches:
           - name: operator
             env:
             - name: PGO_FEATURE_GATES
-              value: ""
+              value: "TablespaceVolumes=true"
#+end_src


#+begin_src sh
  kubectl apply --server-side -k kustomize/install/default/
#+end_src

* Request the volume

"Once you request those tablespaceVolumes, PGO takes care of creating
(or reusing) those volumes, including mounting them to the pod at a
known path (/tablespaces/NAME) and adding them to the necessary
containers."


    dataVolumeClaimSpec:
      volumeName: pvc-1001c17d-c137-4f78-8505-be4b26136924 # A preexisting volume you want to reuse for PGDATA
      accessModes:
        - 'ReadWriteOnce'
      resources:
        requests:
          storage: 1Gi
    tablespaceVolumes:
      storageClassName: ecm-shared
      - name: user
        dataVolumeClaimSpec:
          storageClassName: ecm-shared
          accessModes:
            - 'ReadWriteOnce'
          resources:
            requests:
              storage: 1Gi
          volumeName: pvc-3fea1531-617a-4fff-9032-6487206ce644 # A preexisting volume you want to use for this tablespace

#+RESULTS:

* HACKING

disk use

7.2G    /var/lib/docker/volumes/minikube/_data/hostpath-provisioner/postgres-operator/second-ecm-test-instance1-rxfp-pgdata/
3.4G    /var/lib/docker/volumes/minikube/_data/hostpath-provisioner/postgres-operator/second-ecm-test-repo2/

* Custom image


* Getting Started

First we need the examples.

#+begin_src sh
  git remote add -f postgres-operator-examples git@github.com:CrunchyData/postgres-operator-examples.git
  git subtree add --prefix=postgres-operator/ --squash postgres-operator-examples main
#+end_src

#+begin_quote
By default, Crunchy Postgres for Kubernetes deploys with debug logging
turned on. If you wish to disable this, you need to set the
CRUNCHY_DEBUG environmental variable to "false" that is found in the
kustomize/install/manager/manager.yaml file. Alternatively, you can
add the following to your kustomize/install/manager/kustomization.yaml
to disable debug logging:

patchesStrategicMerge:
- |-
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: pgo
  spec:
    template:
      spec:
        containers:
        - name: operator
          env:
          - name: CRUNCHY_DEBUG
            value: "false"

You can also create additional Kustomize overlays to further patch and customize the installation according to your specific needs.

#+end_quote

Now install the namespace and operator.

#+begin_src sh
  cd postgres-operator/
  sudo kubectl apply -k kustomize/install/namespace
  kubectl apply --server-side -k kustomize/install/default
#+end_src

If it's working the following will tell you.

#+begin_src sh
$ sudo kubectl -n postgres-operator get pods --selector=postgres-operator.crunchydata.com/control-plane=postgres-operator --field-selector=status.phase=Running
NAME                   READY   STATUS    RESTARTS   AGE
pgo-6dc775b4df-hb788   1/1     Running   0          6m25s
#+end_src

** Create a Postgres Cluster

Using the example in the kustomize/postgres directory, all we have to
do is run:

#+begin_src sh
  sudo kubectl apply -k kustomize/postgres
  # => postgrescluster.postgres-operator.crunchydata.com/hippo created
#+end_src

And the view the status 
#+begin_src sh
sudo kubectl -n postgres-operator describe postgresclusters.postgres-operator.crunchydata.com hippo
sudo kubectl -n postgres-operator get pods --selector=postgres-operator.crunchydata.com/cluster=hippo,postgres-operator.crunchydata.com/instance
#+end_src

** Connect to a Postgres Cluster

Connect Using a Port-Forward

In a new terminal, create a port forward. If you are using Bash, you
can run the following commands:

#+begin_src sh
  alias kubectl="sudo kubectl"
  PG_CLUSTER_PRIMARY_POD=$(kubectl get pod -n postgres-operator -o name -l postgres-operator.crunchydata.com/cluster=hippo,postgres-operator.crunchydata.com/role=master)
  kubectl -n postgres-operator port-forward "${PG_CLUSTER_PRIMARY_POD}" 5433:5432
#+end_src

#+begin_src sh
  PG_CLUSTER_USER_SECRET_NAME=hippo-pguser-hippo

  export PGPASSWORD=$(kubectl get secrets -n postgres-operator "${PG_CLUSTER_USER_SECRET_NAME}" -o go-template='{{.data.password | base64decode}}')

  export PGUSER=$(kubectl get secrets -n postgres-operator "${PG_CLUSTER_USER_SECRET_NAME}" -o go-template='{{.data.user | base64decode}}') 
  export PGDATABASE=$(kubectl get secrets -n postgres-operator "${PG_CLUSTER_USER_SECRET_NAME}" -o go-template='{{.data.dbname | base64decode}}') 
  psql -h localhost -p 5433
#+end_src

** Re-create with no users and no database

#+begin_src sh
  cp -av kustomize/postgres/ kustomize/first-ecm-test
#+end_src

Let's spec more like our needs.

#+begin_src sh :results verbatim :wrap src diff
        cd postgres-operator
        diff kustomize/postgres/postgres.yaml kustomize/first-ecm-test/postgres.yaml
#+end_src

#+begin_src diff
4c4
<   name: hippo
---
>   name: first-ecm-test
8c8
<   postgresVersion: 17
---
>   postgresVersion: 16
10,12c10
<     - name: hippo
<       databases:
<         - zoo
---
>     - name: postgres
20c18
<             storage: 1Gi
---
>             storage: 4Gi
31c29
<                 storage: 1Gi
---
>                 storage: 4Gi
#+end_src

And give it a go!

#+begin_src sh
  kubectl apply -k kustomize/first-ecm-test
#+end_src

status?

#+begin_src sh
kubectl -n postgres-operator describe postgresclusters.postgres-operator.crunchydata.com first-ecm-test
kubectl -n postgres-operator get pods --selector=postgres-operator.crunchydata.com/cluster=first-ecm-test,postgres-operator.crunchydata.com/instance
#+end_src

Looks good. Does it work?

#+begin_src sh
  export PG_CLUSTER_PRIMARY_POD=$(kubectl get pod -n postgres-operator -o name -l postgres-operator.crunchydata.com/cluster=first-ecm-test,postgres-operator.crunchydata.com/role=master)
  export PGUSER=postgres
  unset PGDATABASE
  export PGPASSWORD=$(kubectl get secrets -n postgres-operator first-ecm-test-pguser-postgres -o go-template='{{.data.password | base64decode}}')
#+end_src

Forward and see.

#+begin_src sh
  kubectl -n postgres-operator port-forward "${PG_CLUSTER_PRIMARY_POD}" 5433:5432 &
  psql -h localhost -p 5433
#+end_src

*** Try the ECM dump!

#+begin_src sh
  psql -h localhost -p 5433 --echo-all -f ~/db-dump-2025-10-10.sql
#+end_src

That fails after the first bit with:

#+begin_quote
psql:/home/ecm/db-dump-2025-10-10.sql:3391: error: \connect: connection to server at "localhost" (::1), port 5433 failed: FATAL:  password authentication failed for user "postgres"
connection to server at "localhost" (::1), port 5433 failed: FATAL:  no pg_hba.conf entry for host "127.0.0.1", user "postgres", database "template1", no encryption
#+end_quote

That actually does make sense as the dump also redoes that user's
password.

But I already looked into authentication so we go from there.

** Allow all for the Auth so the dump takes

Delete the last few tries.

#+begin_src sh
  kubectl delete -k kustomize/postgres/
  kubectl delete -k kustomize/first-ecm-test
#+end_src

Copy to a new one.

#+begin_src sh
  cp -av kustomize/first-ecm-test/ kustomize/second-ecm-test
#+end_src

Edit it for the new HBA.

#+begin_src sh :results verbatim :wrap src diff
        cd postgres-operator
        diff  kustomize/first-ecm-test/postgres.yaml kustomize/second-ecm-test/postgres.yaml
#+end_src

#+begin_src diff
4c4
<   name: first-ecm-test
---
>   name: second-ecm-test
10a11,14
>   authentication:
>     rules:
>       - hba: "hostssl all all all trust"
>       - hba: "host all all all trust"
29c33
<                 storage: 4Gi
---
>                 storage: 1Gi
#+end_src

The storage is because I'm running out of space on the minikube.

#+begin_src sh
  kubectl apply -k kustomize/second-ecm-test
#+end_src

#+begin_src sh
  export PG_CLUSTER_NAME=second-ecm-test
  export PG_CLUSTER_PRIMARY_POD=$(kubectl get pod -n postgres-operator -o name -l postgres-operator.crunchydata.com/cluster=$PG_CLUSTER_NAME,postgres-operator.crunchydata.com/role=master)
  export PGUSER=postgres
  unset PGDATABASE
  export PGPASSWORD=$(kubectl get secrets -n postgres-operator $PG_CLUSTER_NAME-pguser-postgres -o go-template='{{.data.password | base64decode}}')
#+end_src

Forward and see.

#+begin_src sh
  kubectl -n postgres-operator port-forward "${PG_CLUSTER_PRIMARY_POD}" 5433:5432 &
  # port forwarding takes a bit
  sleep 2;
  psql -h localhost -p 5433
#+end_src

*** Try the ECM dump!

#+begin_src sh
  psql -h localhost -p 5433 --echo-all -f ~/db-dump-2025-10-10.sql
#+end_src

It actually worked. Nice!

** Copy to repo root

#+begin_src sh
  cp -av postgres-operator/kustomize/install ./kustomize/
  cp -av postgres-operator/kustomize/second-ecm-test kustomize/ecm-db
#+end_src

** Conclusion

Now that we have the start we are no longer getting started.





* Azure & Crunchy Bridge

https://marketplace.microsoft.com/en-us/product/saas/crunchydatasolutionsinc1648056888353.bridge-prod?tab=Overview
